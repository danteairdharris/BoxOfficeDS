{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f77a7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\anaconda\\lib\\site-packages (from requests) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a7fcd9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5019c010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "https://en.wikipedia.org/wiki/Harry_Potter_and_the_Philosopher%27s_Stone_(film)\n",
      "'NoneType' object is not subscriptable\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "https://en.wikipedia.org/wiki/The_Dukes_of_Hazzard_(film)\n",
      "'NoneType' object is not subscriptable\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "https://en.wikipedia.orghttps://nl.wikipedia.org/wiki/Morrison_krijgt_een_zusje\n",
      "HTTPSConnectionPool(host='en.wikipedia.orghttps', port=443): Max retries exceeded with url: //nl.wikipedia.org/wiki/Morrison_krijgt_een_zusje (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002635F4DC640>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "https://en.wikipedia.orghttps://nl.wikipedia.org/wiki/Hoe_overleef_ik_mezelf%3F_(film)\n",
      "HTTPSConnectionPool(host='en.wikipedia.orghttps', port=443): Max retries exceeded with url: //nl.wikipedia.org/wiki/Hoe_overleef_ik_mezelf%3F_(film) (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002635DD71E50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n"
     ]
    }
   ],
   "source": [
    "## Save list_movie_data in json\n",
    "def save_data(filename, data):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "## Load file data \n",
    "def load_data(filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "#function for scraping infobox\n",
    "def scrape(url):\n",
    "    page = requests.get(url)\n",
    "    toScrape = bs(page.content, 'html.parser')\n",
    "    movie_details = toScrape.find(class_='infobox vevent')\n",
    "    rows = movie_details.find_all('tr') \n",
    "    movie_data = {}\n",
    "\n",
    "    movie_data['Title'] = rows[0].find('th').get_text()\n",
    "    clean_tags(movie_details)\n",
    "    for i, row in enumerate(rows):\n",
    "        try:\n",
    "            if i <= 1:\n",
    "                continue\n",
    "            elif row.find('th').get_text() == 'Based on':\n",
    "                continue\n",
    "            else:\n",
    "                column = row.find('th').get_text(' ', strip=True)\n",
    "                movie_data[column] = clean(row)\n",
    "        except:\n",
    "            pass\n",
    "    return movie_data\n",
    "\n",
    "#remove troublesome tags\n",
    "def clean_tags(content):\n",
    "    t = ['sup', 'span']\n",
    "    tags = content.find_all(t)\n",
    "    for tag in tags:\n",
    "        tag.decompose()\n",
    "\n",
    "#function to clean data scraped from wikipedia infobox\n",
    "def clean(row):\n",
    "    if row.find('th').get_text() == 'Release date':\n",
    "        date = row.find('td').get_text(' ', strip=True).split('(')[0].replace('\\xa0', ' ')\n",
    "        return date\n",
    "    elif row.find('br'):\n",
    "        return [text for text in row.find('td').stripped_strings]\n",
    "    elif row.find('li'):\n",
    "        return [li.get_text(' ', strip=True).replace('\\xa0', ' ') for li in row.find_all('li')]\n",
    "    else:\n",
    "        if '$' in row.find('td').get_text():\n",
    "            if 'million' in row.find('td').get_text():\n",
    "                if '-' in row.find('td').get_text():\n",
    "                    number = float(row.find('td').get_text().split('-')[0].replace('$',''))\n",
    "                number = float(row.find('td').get_text().split(' ')[0].replace('$',''))\n",
    "                money = number * (10**6)\n",
    "                return money\n",
    "            elif 'billion' in row.find('td').get_text():\n",
    "                if '-' in row.find('td').get_text():\n",
    "                    number = float(row.find('td').get_text().split('-')[0].replace('$',''))\n",
    "                number = float(row.find('td').get_text().split(' ')[0].replace('$',''))\n",
    "                money = number * (10**9)\n",
    "                return money\n",
    "       \n",
    "    return row.find('td').get_text().strip().replace('\\n', ',').replace('\\xa0', ' ')\n",
    "\n",
    "# load in table of WBMovies \n",
    "page = requests.get('https://en.wikipedia.org/wiki/List_of_Warner_Bros._films_(2000%E2%80%932009)')\n",
    "content = bs(page.content, 'html.parser')\n",
    "table_rows = content.select('.wikitable.sortable i')\n",
    "\n",
    "#loop through table rows and scrape entry given the url\n",
    "#Append movie info each iteration\n",
    "list_movie_data = []\n",
    "for i,row in enumerate(table_rows):\n",
    "    if i % 10 == 0:\n",
    "        print(i);\n",
    "    try:\n",
    "        path = row.find('a')['href']\n",
    "        movie_url = 'https://en.wikipedia.org' + path\n",
    "        list_movie_data.append(scrape(movie_url))\n",
    "    except Exception as e:\n",
    "        print(movie_url)\n",
    "        print(e)\n",
    "    \n",
    "save_data('WB_movie_data.json', list_movie_data)\n",
    "\n",
    "## still need to change key to 'Running time (min)' then remove 'minutes' from value \n",
    "## convert date to datetime object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02547280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
